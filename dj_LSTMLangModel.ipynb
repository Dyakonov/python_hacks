{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dyakonov/python_hacks/blob/master/dj_LSTMLangModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr_HDBrT-XlW"
      },
      "source": [
        "# Языковая модель на LSTM\n",
        "\n",
        "по мотивам https://atcold.github.io/pytorch-Deep-Learning/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5cOUtpz-XlX",
        "outputId": "a9a97d48-b9cf-4c13-a072-c67b1323a3d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.rnn as rnn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# import shakespeare_data as sh\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYT7Q6Vd-XlY"
      },
      "source": [
        "## Загрузить данные"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "VSoPFZvH-XlY",
        "outputId": "54991443-96e5-48d4-f280-786e10cc4a48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "александр сергеевич пушкин евгений онегин роман в стихах ,. проникнутый тщеславием, он обладал сверх того еще особенной гордостью, которая побуждает признаваться с одинаковым равнодушием в своих как добрых, так и дурных поступках, следствие чувства превосходства, быть может мнимого.из частного письма фр. мысля гордый свет забавить, вниманье дружбы возлюбя, хотел бы я тебе представить залог достойнее тебя, достойнее души прекрасной, святой исполненной мечты, поэзии живой и ясной, высоких дум и простоты; но так и быть рукой пристрастной прими собранье пестрых глав, полусмешных, полупечальных, простонародных, идеальных, небрежный плод моих забав, бессонниц, легких вдохновений, незрелых и увядших лет, ума холодных наблюдений и сердца горестных замет. глава первая и жить торопится, и чувствовать спешит. князь вяземский эпиграф взят из стихотворения п. а. вяземского первый снег. мой дядя самых честных правил, когда не в шутку занемог, он уважать себя заставил и лучше выдумать не мог. его пример другим наука; но, боже мой, какая скука с больным сидеть и день и ночь, не отходя ни шагу прочь! какое низкое коварство полуживого забавлять, ему подушки поправлять, печально подносить лекарство, вздыхать и думать про себя: когда же черт возьмет тебя! так думал молодой повеса, летя в пыли на почтовых, всевышней волею зевеса наследник всех своих родных. друзья людмилы и руслана! с героем моего романа без предисловий, сей же час позвольте познакомить вас: онегин, добрый мой приятель, родился на брегах невы, где, может быть, родились вы или блистали, мой читатель; там некогда гулял и я: но вреден север для меня писано в бесарабии. служив отлично благородно, долгами жил его отец, давал три бала ежегодно и промотался наконец. судьба евгения хранила: сперва за ним ходила, потом ее сменил; ребенок был резов, но мил. , француз убогой, чтоб не измучилось дитя, учил его всему шутя, не докучал моралью строгой, слегка за шалости бранил и в летний сад гулять водил. когда же юности мятежной приш\n"
          ]
        }
      ],
      "source": [
        "# filename = 'onegin_small.txt'\n",
        "filename = 'onegin.txt'\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "def read_corpus(filename):\n",
        "    # r = re.compile(\"[а-яА-Я .!,;:]+\")\n",
        "    lines = []\n",
        "    with open(filename, 'r', encoding='Windows-1251', errors='ignore') as f:\n",
        "        for pos, line in enumerate(f):\n",
        "            # line = line.replace(\"\\t\", \"\").replace(\"\\n\", \" \")\n",
        "            #line = ''.join([c for c in filter(r.match, line)]) # оставить русские буквы\n",
        "            #\n",
        "            line = re.sub('[^а-яА-Я .!,;:]+', ' ', line.replace(\"\\t\", \"\").replace(\"\\n\", \" \")).strip().lower()\n",
        "            line = re.sub(\" +\", \" \", line) # схлопываем пробелы\n",
        "            line = line.replace(\" .\", \".\")\n",
        "            line = line.replace(\" ,\", \",\")\n",
        "            line = line.replace(\" !\", \"!\")\n",
        "            line = re.sub(\"[.]+\", \".\", line)\n",
        "            line = re.sub(\"[,]+\", \",\", line)\n",
        "            line = re.sub(\"[!]+\", \"!\", line)\n",
        "            if len(line.strip()) > 0:\n",
        "                lines.append(line)\n",
        "    corpus = \" \".join(lines)\n",
        "    return corpus\n",
        "\n",
        "corpus = read_corpus(filename)\n",
        "print (corpus[:2000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-_7Y72f-XlY",
        "outputId": "a2c1f6bb-59bd-45bd-f39d-7562276a45ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Число символов в корпусе: 147224\n",
            "Число уникальных символов: 38\n",
            "corpus_array.shape: (147224,)\n"
          ]
        }
      ],
      "source": [
        "def get_charmap(corpus):\n",
        "    chars = list(set(corpus))\n",
        "    chars.sort()\n",
        "    charmap = {c: i for i, c in enumerate(chars)}\n",
        "    return chars, charmap\n",
        "\n",
        "\n",
        "def map_corpus(corpus, charmap):\n",
        "    return np.array([charmap[c] for c in corpus], dtype=np.int64)\n",
        "\n",
        "\n",
        "def to_text(line, charset):\n",
        "    return \"\".join([charset[c] for c in line])\n",
        "\n",
        "print(f\"Число символов в корпусе: {len(corpus)}\")\n",
        "chars, charmap = get_charmap(corpus)\n",
        "charcount = len(chars)\n",
        "print(f\"Число уникальных символов: {len(chars)}\")\n",
        "corpus_array = map_corpus(corpus, charmap)\n",
        "print(f\"corpus_array.shape: {corpus_array.shape}\")\n",
        "\n",
        "# Число символов в корпусе: 158663\n",
        "# Число уникальных символов: 148\n",
        "# corpus_array.shape: (158663,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVxZUUtV-XlY",
        "outputId": "c0173b1a-b9ec-48ee-ece0-7cc1a5eced90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ! , . : ; а б в г д е ж з и й к л м н о п р с т у ф х ц ч ш щ ъ ы ь э ю я\n"
          ]
        }
      ],
      "source": [
        "print (' '.join(chars)) # символы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uu7TGnaS-XlY",
        "outputId": "b509962b-da6f-4bda-a372-6981d9709134"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{' ': 0, '!': 1, ',': 2, '.': 3, ':': 4, ';': 5, 'а': 6, 'б': 7, 'в': 8, 'г': 9, 'д': 10, 'е': 11, 'ж': 12, 'з': 13, 'и': 14, 'й': 15, 'к': 16, 'л': 17, 'м': 18, 'н': 19, 'о': 20, 'п': 21, 'р': 22, 'с': 23, 'т': 24, 'у': 25, 'ф': 26, 'х': 27, 'ц': 28, 'ч': 29, 'ш': 30, 'щ': 31, 'ъ': 32, 'ы': 33, 'ь': 34, 'э': 35, 'ю': 36, 'я': 37}\n"
          ]
        }
      ],
      "source": [
        "print (charmap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wF577ue0-XlZ"
      },
      "outputs": [],
      "source": [
        "# текст -> последовательности фиксированной длины\n",
        "# плохая версия!!!\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "\n",
        "    def __init__(self, text, seq_len = 200):\n",
        "        n_seq = len(text) // seq_len\n",
        "        text = text[:n_seq * seq_len]\n",
        "        self.data = torch.tensor(text).view(-1,seq_len)\n",
        "\n",
        "    def __getitem__(self,i):\n",
        "        txt = self.data[i]\n",
        "        return txt[:-1], txt[1:] # метки - это те же последовательности, сдвинутые на 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.size(0)\n",
        "\n",
        "# используется в DataLoader - список последовательностей в батч\n",
        "# ответ: seq_len x batch_size\n",
        "def collate(seq_list):\n",
        "    inputs = torch.cat([s[0].unsqueeze(1) for s in seq_list], dim=1)\n",
        "    targets = torch.cat([s[1].unsqueeze(1) for s in seq_list], dim=1)\n",
        "    return inputs, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zW0cIQ3j-XlZ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "более хорошая версия\n",
        "\"\"\"\n",
        "class TextDataset(Dataset):\n",
        "\n",
        "    def __init__(self, text, seq_len = 200):\n",
        "        self.len = len(text) - seq_len + 1\n",
        "        self.data = []\n",
        "        self.seq_len = seq_len\n",
        "        for i in range(self.len):\n",
        "            self.data.append(torch.tensor(text[i: i+self.seq_len]))\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        #line = self.data[i: i+self.seq_len]\n",
        "        #line = torch.tensor(line) # это плохо\n",
        "        line = self.data[i]\n",
        "        return line[:-1].to(DEVICE), line[1:].to(DEVICE)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GD7qZAN--XlZ"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "class CharLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, nlayers):\n",
        "        super(CharLanguageModel,self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.nlayers = nlayers\n",
        "        self.embedding = nn.Embedding(vocab_size,\n",
        "                                      embed_size) # Embedding layer\n",
        "        self.rnn = nn.LSTM(input_size = embed_size,\n",
        "                           hidden_size=hidden_size,\n",
        "                           num_layers=nlayers) # Recurrent network\n",
        "        self.scoring = nn.Linear(hidden_size, vocab_size) # Projection layer\n",
        "\n",
        "    def forward(self, seq_batch): # L x N\n",
        "        # returns 3D logits\n",
        "        batch_size = seq_batch.size(1) # здесь это размерность 1\n",
        "        embed = self.embedding(seq_batch) # L x N x E\n",
        "        hidden = None\n",
        "        output_lstm, hidden = self.rnn(embed, hidden) # L x N x H\n",
        "        output_lstm_flatten = output_lstm.view(-1, self.hidden_size) # (L*N) x H\n",
        "        output_flatten = self.scoring(output_lstm_flatten) #(L*N) x V\n",
        "        return output_flatten.view(-1, batch_size, self.vocab_size)\n",
        "\n",
        "    def generate(self, seq, n_words): # L x V\n",
        "        # жадный поиск для генерации слов\n",
        "        generated_words = []\n",
        "        embed = self.embedding(seq).unsqueeze(1) # L x 1 x E\n",
        "        hidden = None\n",
        "        output_lstm, hidden = self.rnn(embed, hidden) # L x 1 x H\n",
        "        output = output_lstm[-1] # 1 x H\n",
        "        scores = self.scoring(output) # 1 x V\n",
        "        _, current_word = torch.max(scores, dim=1) # 1 x 1\n",
        "        generated_words.append(current_word)\n",
        "        if n_words > 1:\n",
        "            for i in range(n_words-1):\n",
        "                embed = self.embedding(current_word).unsqueeze(0) # 1 x 1 x E\n",
        "                output_lstm, hidden = self.rnn(embed, hidden) # 1 x 1 x H\n",
        "                output = output_lstm[0] # 1 x H\n",
        "                scores = self.scoring(output) # V\n",
        "                _,current_word = torch.max(scores, dim=1) # 1\n",
        "                generated_words.append(current_word)\n",
        "        return torch.cat(generated_words, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKyS-MEm-XlZ"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, optimizer, train_loader, val_loader):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    criterion = criterion.to(DEVICE)\n",
        "    train_loss = 0\n",
        "    before = time.time()\n",
        "    print(\"training\", len(train_loader), \"number of batches\")\n",
        "    for batch_idx, (inputs,targets) in enumerate(train_loader):\n",
        "        if batch_idx == 0:\n",
        "            first_time = time.time()\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        targets = targets.to(DEVICE)\n",
        "        outputs = model(inputs) # 3D\n",
        "        loss = criterion(outputs.view(-1,outputs.size(2)),targets.view(-1)) # Loss of the flattened outputs\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        #if batch_idx == 0:\n",
        "        #    print(\"Time elapsed\", time.time() - first_time)\n",
        "\n",
        "        #if batch_idx % 500 == 0 and batch_idx != 0:\n",
        "        #    after = time.time()\n",
        "        #    print(\"Time: \", after - before)\n",
        "        #    print(\"Loss per word: \", loss.item() / batch_idx)\n",
        "        #    print(\"Perplexity: \", np.exp(loss.item() / batch_idx))\n",
        "        #    after = before\n",
        "\n",
        "    train_loss = train_loss / batch_idx\n",
        "\n",
        "    val_loss = 0\n",
        "    batch_id = 0\n",
        "    for inputs,targets in val_loader:\n",
        "        batch_id += 1\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        targets = targets.to(DEVICE)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.view(-1,outputs.size(2)), targets.view(-1))\n",
        "        val_loss += loss.item()\n",
        "    val_lpw = val_loss / batch_id\n",
        "    # print(\"\\nValidation loss per word:\",val_lpw)\n",
        "    print(\"Train perplexity :\", np.exp(train_loss))\n",
        "    print(\"Validation perplexity :\", np.exp(val_loss / batch_id))\n",
        "    return val_lpw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc9E1D1q-XlZ"
      },
      "outputs": [],
      "source": [
        "#model = CharLanguageModel(charcount, 256, 256,3)\n",
        "model = CharLanguageModel(vocab_size=charcount,\n",
        "                          embed_size=256,\n",
        "                          hidden_size=256,\n",
        "                          nlayers=2)\n",
        "model = model.to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr=0.01, weight_decay=1e-7) #, # lr=0.001,\n",
        "                             #weight_decay=1e-6)\n",
        "\n",
        "split = 120000\n",
        "train_dataset = TextDataset(corpus_array[:split], seq_len=100)\n",
        "val_dataset = TextDataset(corpus_array[split:], seq_len=100)\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=64, collate_fn = collate)\n",
        "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=64, collate_fn = collate, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elQpQNEe-XlZ"
      },
      "outputs": [],
      "source": [
        "#train_dataset = TextDataset(shakespeare_array, seq_len = 10)\n",
        "#train_loader = DataLoader(train_dataset, shuffle=True, batch_size=1, collate_fn = collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSGHOPo--XlZ",
        "outputId": "c1131169-c035-4748-8335-db1ee0b16308"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(119901, 27125)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset.__len__(), val_dataset.__len__()\n",
        "# (1200, 386)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hlve73c-XlZ"
      },
      "outputs": [],
      "source": [
        "# i = 0\n",
        "# for i1, i2 in val_loader:\n",
        "#     i = i + 1\n",
        "#     if (i>10): break;\n",
        "#     # print(i1.shape, i2.shape)\n",
        "#     print(i1, i2)\n",
        "\n",
        "128 3 0.01\n",
        "Train perplexity : 1.708431299024099\n",
        "Validation perplexity : 68.95001547975727\n",
        "\n",
        "256 3 lr=0.01, weight_decay=1e-7\n",
        "Train perplexity : 1.6952816075689339\n",
        "Validation perplexity : 55.76105459243429\n",
        "\n",
        "256 2 lr=0.01, weight_decay=1e-7\n",
        "Train perplexity : 1.9566967733083707\n",
        "Validation perplexity : 32.95505913167213"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc_pd3kk-XlZ",
        "outputId": "bf04fc1a-d0c7-4eed-bbdb-448cc57f9fb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training 1874 number of batches\n",
            "Train perplexity : 3.100737592348649\n",
            "Validation perplexity : 20.92226048805598\n",
            "training 1874 number of batches\n",
            "Train perplexity : 1.68108752638697\n",
            "Validation perplexity : 44.035060143245296\n"
          ]
        }
      ],
      "source": [
        "for i in range(2):\n",
        "    train_epoch(model=model,\n",
        "                optimizer=optimizer,\n",
        "                train_loader=train_loader,\n",
        "                val_loader=val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WtciWCT-XlZ"
      },
      "outputs": [],
      "source": [
        "def generate(model, seed, nwords):\n",
        "    seq = map_corpus(seed, charmap)\n",
        "    seq = torch.tensor(seq).to(DEVICE)\n",
        "    out = model.generate(seq, nwords)\n",
        "    return to_text(out.cpu().detach().numpy(), chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hciqG-mN-XlZ",
        "outputId": "07b643a8-af9b-4178-c80a-9bbe99ba9c02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " значит видеть свет! где ж лучше гордым и после важно повторять одно, стараться вас задригалы иль пр\n"
          ]
        }
      ],
      "source": [
        "print(generate(model, \"онегин встал и подошел, сказав, что\", 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8Q05TkI-XlZ",
        "outputId": "e1bb8d44-8c2e-4830-e58b-da67d38f0450"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " занеможественной мечты, поэзии живой и кума ему не наши внемя. тут же полуравет, и вдруг нетвал, пр\n"
          ]
        }
      ],
      "source": [
        "print(generate(model, \"мой дядя самых честных правил, когда не в шутку\", 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qtp4tIl-Xla"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KE139E0x-Xla"
      },
      "outputs": [],
      "source": [
        "seq = map_corpus(corpus[500:530], charmap) # \"Высоких дум и простоты\"\n",
        "seq = torch.tensor(seq).to(DEVICE)\n",
        "out = model.generate(seq, 100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAR_fVDY-Xla",
        "outputId": "59ce5e54-9618-45e4-8c85-dee9003708d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'александр сергеевич пушкин евгений онегин роман в стихах ,. проникнутый тщеславием, он обладал сверх того еще особенной гордостью, которая побуждает признаваться с одинаковым равнодушием в своих как добрых, так и дурных поступках, следствие чувства превосходства, быть может мнимого.из частного письма фр. мысля гордый свет забавить, вниманье дружбы возлюбя, хотел бы я тебе представить залог достойнее тебя, достойнее души прекрасной, святой исполненной мечты, поэзии живой и ясной, высоких дум и простоты; но так и быть рукой пристрастной прими собранье пестрых глав, полусмешных, полупечальных, простонародных, идеальных, небрежный плод моих забав, бессонниц, легких вдохновений, незрелых и увядших лет, ума холодных наблюдений и сердца горестных замет. глава первая и жить торопится, и чувствовать спешит. князь вяземский эпиграф взят из стихотворения п. а. вяземского первый снег. мой дядя самых честных правил, когда не в шутку занемог, он уважать себя заставил и лучше выдумать не мог. его при'"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus[:1000] # 500:1530]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEk6k6WM-Xla",
        "outputId": "0df0524a-a0a4-49bb-b9a0-ea81771af85f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([21, 20, 23, 24,  6, 17, 34, 19, 20,  0, 21, 20, 23, 24,  6, 17, 34, 19,\n",
              "        20,  0, 21, 20, 23, 24,  6, 17, 34, 19, 20,  0, 21, 20, 23, 24,  6, 17,\n",
              "        34, 19, 20,  0, 21, 20, 23, 24,  6, 17, 34, 19, 20,  0, 21, 20, 23, 24,\n",
              "         6, 17, 34, 19, 20,  0, 21, 20, 23, 24,  6, 17, 34, 19, 20,  0, 21, 20,\n",
              "        23, 24,  6, 17, 34, 19, 20,  0, 21, 20, 23, 24,  6, 17, 34, 19, 20,  0,\n",
              "        21, 20, 23, 24,  6, 17, 34, 19, 20,  0], device='cuda:0')"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PltYvTZW-Xla",
        "outputId": "6350a55a-3af4-4dbd-ca9f-532f76f099bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "148"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "charcount"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}